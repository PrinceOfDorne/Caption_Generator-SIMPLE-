{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f05a50bbddea764801ebf495487cdec1be774790"
   },
   "outputs": [],
   "source": [
    "descriptions = pickle.load(open('cleaned_descriptions.p', 'rb'))\n",
    "descriptions.pop('2258277193_586949ec62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0bae71dbd42da9f14ddd890e6e8cf77cc7ea14a"
   },
   "outputs": [],
   "source": [
    "len(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2729dc8e1635e6272bc6a9fb02644717501b116"
   },
   "outputs": [],
   "source": [
    "def load_photo_features(filename, dataset):\n",
    "\t# load all features\n",
    "\tall_features = pickle.load(open(filename, 'rb'))\n",
    "\t# filter features\n",
    "\tfeatures = {k: all_features[k] for k in dataset.keys()}\n",
    "\treturn features\n",
    "\n",
    "features = load_photo_features('Clean_features.pkl', descriptions)\n",
    "print('Photos: train=%d' % len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47cf9d7098e333f1d329b62581508b5ea9ed56a0"
   },
   "outputs": [],
   "source": [
    "def to_lines(descriptions):\n",
    "\tall_desc = list()\n",
    "\tfor key in descriptions.keys():\n",
    "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
    "\treturn all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1efa3bab2a87791680b5531ddd8718996bcd7d9"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def create_tokenizer(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\ttokenizer = Tokenizer()\n",
    "\ttokenizer.fit_on_texts(lines)\n",
    "\treturn tokenizer\n",
    " \n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "\tlines = to_lines(descriptions)\n",
    "\treturn max(len(d.split()) for d in lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c2cce28fadf4b45976b1288701bf896675d2d405"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo):\n",
    "\tX1, X2, y = list(), list(), list()\n",
    "\t# walk through each description for the image\n",
    "\tfor desc in desc_list:\n",
    "\t\t# encode the sequence\n",
    "\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n",
    "\t\t# split one sequence into multiple X,y pairs\n",
    "\t\tfor i in range(1, len(seq)):\n",
    "\t\t\t# split into input and output pair\n",
    "\t\t\tin_seq, out_seq = seq[:i], seq[i]\n",
    "\t\t\t# pad input sequence\n",
    "\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "\t\t\t# encode output sequence\n",
    "\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "\t\t\t# store\n",
    "\t\t\tX1.append(photo)\n",
    "\t\t\tX2.append(in_seq)\n",
    "\t\t\ty.append(out_seq)\n",
    "\treturn np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5194e2e025bba15ec8228db6d4bbeea23c699b62"
   },
   "outputs": [],
   "source": [
    "def define_model(vocab_size, max_length):\n",
    "\t# feature extractor model\n",
    "\tinputs1 = Input(shape=(4096,))\n",
    "\tfe1 = Dropout(0.5)(inputs1)\n",
    "\tfe2 = Dense(256, activation='relu')(fe1)\n",
    "\t# sequence model\n",
    "\tinputs2 = Input(shape=(max_length,))\n",
    "\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "\tse2 = Dropout(0.5)(se1)\n",
    "\tse3 = LSTM(256)(se2)\n",
    "\t# decoder model\n",
    "\tdecoder1 = add([fe2, se3])\n",
    "\tdecoder2 = Dense(256, activation='relu')(decoder1)\n",
    "\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "\t# tie it together [image, seq] [word]\n",
    "\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "\t# compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\t# summarize model\n",
    "\tmodel.summary()\n",
    "\tplot_model(model, to_file='model.png', show_shapes=True)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72c5a0e68321c2b2b12d5922f50b1e32798be954"
   },
   "outputs": [],
   "source": [
    "def data_generator(descriptions, photos, tokenizer, max_length):\n",
    "\t# loop for ever over images\n",
    "\twhile 1:\n",
    "\t\tfor key, desc_list in descriptions.items():\n",
    "\t\t\t# retrieve the photo feature\n",
    "\t\t\tphoto = photos[key][0]\n",
    "\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n",
    "\t\t\tyield [[in_img, in_seq], out_word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36ede9f4a9fade68e20b3b5367f9bdca4870fe2e"
   },
   "outputs": [],
   "source": [
    "train, test = list(), list()\n",
    "i = 0\n",
    "for key in descriptions.keys():\n",
    "    if i<4000:\n",
    "        train.append(key)\n",
    "    else:\n",
    "        test.append(key)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b53f1e7a2694025498c21e652fa846733b8b9ca"
   },
   "outputs": [],
   "source": [
    "train_descriptions, test_descriptions = dict(), dict()\n",
    "for i in train:\n",
    "    train_descriptions[i] = descriptions[i]\n",
    "for i in test:\n",
    "    test_descriptions[i] = descriptions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1bef00026e21689c39b65287ec34a67d883c0489"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(train_descriptions, open(\"train_descriptions.p\", \"wb\"))\n",
    "pickle.dump(test_descriptions, open(\"test_descriptions.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b0891849e1862c6af7888e5fd29a7396afa1791"
   },
   "outputs": [],
   "source": [
    "train_features, test_features = dict(), dict()\n",
    "for i in train:\n",
    "    train_features[i] = features[i]\n",
    "for i in test:\n",
    "    test_features[i] = features[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c10545b98b6575fd1cdfafcb6fea8283564655b0"
   },
   "outputs": [],
   "source": [
    "pickle.dump(train_features, open(\"train_features.p\", \"wb\"))\n",
    "pickle.dump(test_features, open(\"test_features.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b371de1e0e2dcb69a57a46d679d8c33d7ca2a075"
   },
   "outputs": [],
   "source": [
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4d16259216355acad7e48ddebd84c98978c9b8ec"
   },
   "outputs": [],
   "source": [
    "max_length = max_length(train_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1fdfc7612b0fd7431df5d47253ba5a3aba7c18a1"
   },
   "outputs": [],
   "source": [
    "generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "inputs, outputs = next(generator)\n",
    "print(inputs[0].shape)\n",
    "print(inputs[1].shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63c2ef200cca2c731ceb97de5e9e5e6f49b3ce51"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model = define_model(vocab_size, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "088bd6e5af79340893dbdcc164828c0c85401e8c"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "\t# create the data generator\n",
    "\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "\t# fit for one epoch\n",
    "\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "\t# save model\n",
    "\tmodel.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a3afccb897bb9837e0c517d6075ac973133f5dc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
